{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#How to Create a Custom Resnet and Then Optimize it With Optuna\n",
        "## Tutorial by Dustin Wang\n",
        "\n"
      ],
      "metadata": {
        "id": "y_wBllb8Hep4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I learned a lot of the following material from these resources take a look.\n",
        "\n",
        "To learn more about how CNN's work watch this video series by DeepLearningAI on Youtube (https://www.youtube.com/playlist?list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF)\n",
        "\n",
        "To learn more about how SNN's work you can take a look at these python notebook tutorials (https://snntorch.readthedocs.io/en/latest/tutorials/index.html)\n",
        "\n",
        "To learn more about optimization with Optuna, you can check out their page here(https://optuna.org/)\n",
        "\n",
        "Patrick Loeber's Pytorch tutorial 14 on Youtube (https://www.youtube.com/watch?v=pDdP0TFzsoQ&ab_channel=PatrickLoeber)\n",
        "\n",
        "Transfer Learning tutorial on Pytorch Docs\n",
        "(https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)"
      ],
      "metadata": {
        "id": "fTrMbpOwIKAG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disclaimer: The following tutorial is intended for educational purposes. The resulting model does not actually perform well due to the structure of the model."
      ],
      "metadata": {
        "id": "ZH354JYsTKfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction\n"
      ],
      "metadata": {
        "id": "KVcLHJeyA-TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if, for a project, we started by training a residual network to classify different species of jellyfish and then, just by curiosity, added a spiking neural network to the end of the resnet? Would that work at all? How bad would it be? How accurate can we make this combination? Here are the answers in order: \"Not really\", \"Pretty bad\", and \"I got it to about 76% accuracy\".\n",
        "\n",
        "In this tutorial you will learn how to:\n",
        "* Use transfer learning to train convolutional neural networks on image datasets\n",
        "* Plug a static image into a time-varying neural network\n",
        "* Optimize the hyperparameters of a neural network using Optuna"
      ],
      "metadata": {
        "id": "5X89YP9eBQ5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "\n",
        "You will need to install these libraries into your python environment.\n"
      ],
      "metadata": {
        "id": "KCv2xegdE3Zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import snntorch as snn\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import optuna\n",
        "from optuna.trial import TrialState\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from tempfile import TemporaryDirectory"
      ],
      "metadata": {
        "id": "k6xyfRXAFAjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloading\n",
        "\n",
        "If you haven't already, go ahead and download the image dataset linked above from Kaggle. It has a standard file structure that we can use with Pytorch's ImageFolder class to make this part much easier.\n"
      ],
      "metadata": {
        "id": "lt0EPzCvGl_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "pvVcONYmHhf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Here, we set up some data transforms for data augmentation. The ImageFolder class from will automatically create our datasets. If you take a look at the file structure of our data it looks like this:\n",
        "\n",
        "\n",
        "```\n",
        "root/\n",
        "|-- test/\n",
        "|   |-- class 1\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|   |-- class 2\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|\n",
        "|-- train/\n",
        "|   |-- class 1\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|   |-- class 2\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|\n",
        "|-- val/\n",
        "|   |-- class 1\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|   |-- class 2\n",
        "|       |--img1.jpg\n",
        "|       |--img2.jpg\n",
        "|\n",
        "|-- ...\n",
        "```\n",
        "\n",
        "The ImageFolder class takes one folder with images split into class folders within it. So, with some string manipulation, we pass the train and val folders in to get our datasets and then dataloaders in sequence. I set the dataloader batch size to 4."
      ],
      "metadata": {
        "id": "tbfIWNmeH6_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = './data/jellies'\n",
        "#create datasets for the train and validation folder paths\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "#create dataloaders for the train and validation datasets. Batch size set to 4\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "#run on nvidia cuda cores if we got em\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "LZ9FMEsJcewm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining the Model"
      ],
      "metadata": {
        "id": "iq7-AhhxE4jo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define our model now."
      ],
      "metadata": {
        "id": "O1h3qFv8GCOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, resnet, beta):\n",
        "        super(CustomResNet, self).__init__()\n",
        "        self.resnet = resnet\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(256, 6)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk2_rec = []\n",
        "        mem2_rec = []\n",
        "\n",
        "        cur1 = self.resnet(x)\n",
        "\n",
        "        # Forward pass\n",
        "        # Repeatedly pass the image into SNN and record spikes and mem potential of the final layer\n",
        "        # based on number of steps\n",
        "        for step in range(20):\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            spk2_rec.append(spk2)\n",
        "            mem2_rec.append(mem2)\n",
        "\n",
        "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)"
      ],
      "metadata": {
        "id": "kqgGyOMlMjK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This custom network is residual network passed into a spiking neural network. Since the spiking neural network requires time varying data, I just passed in the residual networks output repeatedly for 20 timesteps. For the spiking neural network, I added two layers. Each parent layer contains a fully connected layer and also a leaky-integrate-fire layer. You'll notice that there isn't a fully connected layer to correspond with the first LIF layer and that problem will be addressed in the next section. Beta is the decay rate for the LIF layers. We want to pass that value in as a variable because it is a hyperparameter that we can optimize."
      ],
      "metadata": {
        "id": "OJNIo9HkMss3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "Transfer learning is when you take a pre-trained model and train it again to specialize it for your needs. In our case we are going to take a pre-trained ResNet provided by Pytorch. I chose to freeze all of the pretrained layers so that the parameters are not trained. This way I can speed up computation."
      ],
      "metadata": {
        "id": "GSHlecYsNmqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_Model(beta):\n",
        "    # Load pre-trained\n",
        "    model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
        "\n",
        "    # Freeze all layers\n",
        "    for param in model_ft.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # get number of input ftrs of last fc layer\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "\n",
        "    # Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
        "    # New layers have requires_grad=True by default\n",
        "    model_ft.fc = nn.Linear(num_ftrs, 256)\n",
        "\n",
        "    #run on gpu if we have one\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    # Create custom resnet with SNN layers\n",
        "    return CustomResNet(model_ft, beta)"
      ],
      "metadata": {
        "id": "JkEthFLgODgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ResNet contains one fully connected layer at the very end of the network. I chose to simply alter the output of that layer so that it would fit into the custom network definition."
      ],
      "metadata": {
        "id": "Md8JbzbGOM4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "BrHtVciZPXCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This last function for the training loop is a bit of a monster but it works in conjunction with Optuna. So let's get into how Optuna works."
      ],
      "metadata": {
        "id": "obAS7eDDPahz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Optuna\n",
        "\n",
        "Simply, given a function and a set of parameters to toy with, Optuna will run many trials in order to get the parameters that best suit your needs. In our case, we have a training loop function that results in a model with an accuracy metric. We want to maximize the accuracy metric and we have a set of hyperparameters that we can give Optuna.\n",
        "\n",
        "As for syntax, Optuna takes an __objective(trial)__ function and you can tell it what parameters to use with __trial.suggestInt()__ or __suggestFloat()__ and so on. Then after you've defined your __objective()__ function and given Optuna the parameters you can call a trial run and Optuna will start its work.\n",
        "\n",
        "This is much easier than plugging hyperparameters manually. Optuna will tell you what parameters are best."
      ],
      "metadata": {
        "id": "QS68kKmZPtGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Training Loop"
      ],
      "metadata": {
        "id": "pfmyEawJRV9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is an altered version of the training loop used in the Transfer Learning Tutorial linked above."
      ],
      "metadata": {
        "id": "yfBafsNGaMwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    # hyperparameters\n",
        "    beta = trial.suggest_float(\"beta\", 0, 1)\n",
        "    lr = trial.suggest_float(\"lr\", .00000001, 0.0001, log=True)\n",
        "    num_epochs = 5\n",
        "\n",
        "    # create model\n",
        "    model_ft = create_Model(beta)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=lr)\n",
        "\n",
        "    # Decay learning rate by a factor of 0.1 every 7 epochs\n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=.1)\n",
        "\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model_ft.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model_ft.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model_ft.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "                acc_hist = []\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # forward\n",
        "                    # track history only if in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        spk_rec, mem_rec = model_ft(inputs)\n",
        "\n",
        "                        # Get prediction by rate. Index of most spiked neuron is the prediction.\n",
        "                        _, idx = spk_rec.sum(dim=0).max(1)\n",
        "\n",
        "                        acc = np.mean((labels == idx).detach().cpu().numpy())\n",
        "\n",
        "                        # Count up all the losses over every time step\n",
        "                        loss = torch.zeros((1), dtype=torch.float)\n",
        "                        for step in range(20):\n",
        "                            loss += criterion(mem_rec[step], labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        optimizer_ft.zero_grad()\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer_ft.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    acc_hist.append(acc)\n",
        "                if phase == 'train':\n",
        "                    exp_lr_scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = np.mean(acc_hist)\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model_ft.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        return best_acc\n",
        "        # load best model weights\n",
        "        #model_ft.load_state_dict(torch.load(best_model_params_path))"
      ],
      "metadata": {
        "id": "TrQZavl7RYhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I like this version of the training loop because it saves the best version of our network automatically which I thought was great, so kudos to Sasank Chilamkurthy who wrote the Transfer Learning tutorial.\n",
        "\n",
        "**In order to calculate accuracy**, we use prediction by spike rate. The neuron in the last layer that spikes the most is the networks prediction. Then, we compare that to the label on the image.\n",
        "\n",
        "**In order to calculate loss**, we use the Cross Entropy Loss which will automatically softmax the membrane potential history and get our loss.\n",
        "\n",
        "Lastly, I've basically wrapped the entire training loop in the objective function and I output the best accuracy at the bottom. At the top, we set Optuna to tune the beta and the learning rate hyperparameters. All that's left now is to run a study with Optuna and wait."
      ],
      "metadata": {
        "id": "pV1OcMKqRjH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=100)"
      ],
      "metadata": {
        "id": "fU4nRVvcSP15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Conclusion"
      ],
      "metadata": {
        "id": "W_sn3bdUSSad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, after waiting 4 hours (Ryzen 9 6900HS CPU), Optuna came out with its verdict. The highest accuracy met was 76% with the parameters below."
      ],
      "metadata": {
        "id": "5sSlonKuTeQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters: {'beta':0.990313977538696, 'lr':6.989109944155411e-05}"
      ],
      "metadata": {
        "id": "tWBZ-tLHS-TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In practice, now that we have parameters close enough to the maximum accuracy, we would plug them into the model to save it, but the accuracy is not good enough to warrant that effort. We have, though, answered the question at the beginning of this notebook. Combining a resnet with an SNN to train on a static image dataset does not work well."
      ],
      "metadata": {
        "id": "9psNA4nMUC1z"
      }
    }
  ]
}